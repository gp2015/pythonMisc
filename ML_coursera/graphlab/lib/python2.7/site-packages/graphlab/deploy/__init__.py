"""
GraphLab Create offers multiple ways to work with your data beyond your desktop
or laptop.

- Batch processing with job scheduling in a distributed system, using a
  Hadoop Yarn or EC2 cluster running Dato Distributed.
- Real-time querying and consumption of predictive objects (i.e. models) with
  GraphLab Create Predictive Services.

Explore the detailed functionality via the following API documentation or the
`Deployment chapter of the User Guide
<https://dato.com/learn/userguide/deployment/introduction.html>`_, as well as the
`Gallery <https://dato.com/learn/gallery>`_.
"""

__all__ = ['environments', 'jobs', 'job', 'map_job']

# Artifacts
from _job import Job
from _task import Task as _Task
from environment import _Environment
import _dml_job

# Sessions
import _session
_default_session = _session._open()

environments = _session.ScopedSession(_default_session, _Environment, """
View and manage Environments available in the session. Environments currently
available can be listed, loaded, and deleted using this object.

Examples
--------

.. sourcecode:: python

  >>> my_env = graphlab.deploy.environment.Local("init-ex2-env")
  >>> graphlab.deploy.environments
  Environment(s):
  +-------+--------------+------------------+------------------+
  | Index |     Name     |       Type       | Unsaved changes? |
  +-------+--------------+------------------+------------------+
  |   0   | init-ex2-env | LocalEnvironment |       Yes        |
  +-------+--------------+------------------+------------------+

  # Load an environment by index number.
  >>> environment = graphlab.deploy.environments[0]

  # Load an environment by name.
  >>> environment = graphlab.deploy.environments['init-ex2-env']

  # Delete an environment by name (deleting by index number also supported).
  >>> del graphlab.deploy.environments['init-ex2-env']

""")

jobs = _session.ScopedSession(_default_session, Job, """
View and manage all Jobs in the session. Jobs currently available can be
listed, loaded, deleted, and can be visualized in Canvas using this object.

Examples
--------

.. sourcecode:: python

  # Monitor all jobs
  >>> gl.deploy.jobs
  +-------+-------------+--------------------------+---------------------------+
  | Index | Environment |           Name           |       Creation date       |
  +-------+-------------+--------------------------+---------------------------+
  |   0   |    async    | add-Feb-11-2015-00-39-32 | 2015-02-11 00:39:32+00:00 |
  +-------+-------------+--------------------------+---------------------------+

  # Load a Job by index number:
  >>> job = graphlab.deploy.jobs[0]

  # Load a Job by name:
  >>> job = graphlab.deploy.jobs['job-print-hello-world-task-init-ex3-env-1407900916']

  # Visualize a Job in Canvas.
  >>> job.show()

  # Visualize a list of Jobs in Canvas.
  >>> graphlab.deploy.jobs.show()

  # to delete a Job by index number (deleting by name also supported):
  >>> del graphlab.deploy.jobs[0]
""")

import job
import predictive_service

from _predictive_service._model_predictive_object import ModelPredictiveObject as _ModelPredictiveObject
from _predictive_service._predictive_service import PredictiveService

predictive_services =  _session.ScopedSession(_default_session, PredictiveService, """
View and manage all Predictive Services in the session.

Predictive Services currently available can be listed, loaded, deleted, and
visualized in Canvas using this object. This object represents the portion of the
workbench for Predictive Services that have been defined in GraphLab Create.

Examples
--------

.. sourcecode:: python

    >>> graphlab.deploy.predictive_services
    PredictiveService(s):
    +-------+--------------+---------------------------+------------------+
    | Index |     Name     |            Type           | Unsaved changes? |
    +-------+--------------+---------------------------+------------------+
    |   0   |    testPS    |     PredictiveService     |        No        |
    +-------+--------------+---------------------------+------------------+

    # Load a Predictive Service by index.
    >>> ps = graphlab.deploy.predictive_services[0]

    # Load a Predictive Service by name.
    >>> ps = graphlab.deploy.predictive_services['testPS']

    # Load a Predictive Service by s3 path.
    >>> ps = graphlab.deploy.predictive_service.load("s3://bucket/testPS")

    # Visualize a Predictive Service in Canvas.
    >>> ps.show()

    # Visualize a list of Predictive Services in Canvas.
    >>> graphlab.deploy.predictive_services.show()

    # to remove a Predictive Service from local workspace by index number
    # (deleting by name also supported):
    >>> del graphlab.deploy.predictive_services[0]
    # Note: The above operation does NOT terminate the Predictive Service.
""")

def required_packages(packages):
    """
    Decorator to annotate the set of packages required for executing a custom
    function.

    Parameters
    ----------
    packages: list[str]
        A list that specifies python packages required by the decorated function.
        The dependencies are specified in the format of local file paths or
        distutils, like: ['mysql==0.3.0', 'abc==1.2.3'].
        The package can either be a pypi package, or a local Python egg.
        The egg file has to be installable in a Linux Operating System.

    Returns
    ----------
    A decorator specifying required packages.

    See Also
    --------
    required_files

    Examples
    --------
    This example specifies the names package as a dependency for a custom
    function:

    .. sourcecode:: python

        @graphlab.deploy.required_packages(['names==0.3.0'])
        def my_custom_query(input):
            import names
            return [names.get_first_name() for i in range(input['size'])]

        >>> ps.add('my_custom_query_name', my_custom_query)
        >>> ps.apply_changes()

    To add a local Python egg as dependent package, use its local file path:

    .. sourcecode:: python

        @graphlab.deploy.required_packages(['~/my_package_folder/package1.tar.gz'])
        def my_function():
            import package1
            ...

    You can mix the local file and the pypi package specification:

    .. sourcecode:: python

        @graphlab.deploy.required_packages(['~/my_package_folder/package1.tar.gz', 'names==0.3.0'])
        def my_function():
            import package1
            import names
            ...

    """
    def decorator_without_arguments(func):
        # validate python package dependencies, should be a list of strings
        if not isinstance(packages, list) or \
            any([not isinstance(dependency, str) for dependency in packages]):
            raise TypeError(("python package dependencies has to be a list of"
                      "strings like: ['mysql==1.0.0', 'another==0.2.3']"))
        func.func_dict['required_packages'] = packages
        return func
    return decorator_without_arguments

def required_files(files, pattern='*'):
    ''' Decorator to annotate the set of dependent file(s) required for
    executing a custom predictive object.

    Package required files for remote execution. This ensures that code written
    locally can safely run in remote environments. The files will be packaged
    and shipped to the remote execution environment. The files will be laid
    out in the same directory structure, in the remote machine, as was present
    in the current working directory.

    Parameters
    -----------
    files : list[str] | str
        Files can be one of the following types:

        - If 'files' is a string and points to a directory, then all files under
          the directory with the given pattern will be shipped.
        - If 'files' is a string and points to a file, the only the one file is
          shipped, pattern will be ignored.
        - If 'files' is a list of string, then we treat them as a set of files to
          be shipped.

    pattern: str
        The file name pattern, it is used as a filter to filter out files that
        are not needed.

    See Also
    --------
    required_packages

    Examples
    --------

        To include all files in a given directory.

        .. sourcecode:: python

            from graphlab.deploy import required_files
            @required_files('my_module_directory_name', pattern='*.py')
            def my_function(input):
                # my logic
                return output

        To include a list of files required by the function.

        .. sourcecode:: python

            from graphlab.deploy import required_files
            @required_files(['file1.py', 'file2.py'])
            def my_function(input):
                # my logic
                return output

        To include a mix of files and directories of files.

        .. sourcecode:: python

            from graphlab.deploy import required_files
            @required_files(['file1.py', 'folder1'], pattern='*.py')
            def my_function(input):
                # my logic
                return output

    Notes
    -----
    - Note that all files are going to be recorded as a relative path to the
      current working directory. When shipped to remote machine, the files
      are going to be laid out in exactly the same structure as in your folder.
    '''

    import os as _os

    def _find_files(directory, pattern):
        '''get all files in a given directory with given pattern'''
        import fnmatch as _fnmatch
        for root, dirs, files in _os.walk(directory):
            for basename in files:
                if _fnmatch.fnmatch(basename, pattern):
                    filename = _os.path.join(root, basename)
                    yield filename

    def _read_file_or_directory(base_dir, file_or_dir):
        if _os.path.isfile(file_or_dir):
            relpath = _os.path.relpath(file_or_dir, start=base_dir)
            if '..' in relpath:
                suggested_path = _os.path.split(file_or_dir)[0]
                raise ValueError(\
                    "We use the file's relative path to your current working directory '%s' to"
                    " determine your file's remote location. '%s' is in a path that is not"
                    " under current working directory. Please switch your current working"
                    " directory using os.chdir() to switch to a parent folder of your files"
                    " to be uploaded and then try again."
                    " For example, you may do:\n\n"
                    " \timport os\n"
                    " \tos.chdir('%s')\n" % (base_dir, file_or_dir, suggested_path))
            return {relpath: file_or_dir}
        elif _os.path.isdir(file_or_dir):
            ret = {}
            for f in _find_files(file_or_dir, pattern):
                ret.update(_read_file_or_directory(base_dir, f))
            return ret
        else:
            raise TypeError('"%s" is not a file or directory' % file_or_dir)

    if isinstance(files, basestring):
        files = _os.path.realpath(_os.path.expanduser(files))
        if not _os.path.isdir(files) and not _os.path.isfile(files):
            raise TypeError('"%s" does not point to any file or directory' % files)

        files = [files]

    if not isinstance(files, list) or \
        not all([isinstance(f, basestring) for f in files]):
        raise TypeError('"files" parameter has to be either a file name, directory' \
            ' name or a list of files')

    all_files = {}
    for f in files:
        f = _os.path.realpath(_os.path.expanduser(f))
        all_files.update(_read_file_or_directory(_os.getcwd(), f))

    def decorator_without_arguments(func):
        func.func_dict['required_files'] = all_files
        return func

    return decorator_without_arguments

from _ec2_config import Ec2Config

from ec2_cluster import Ec2Cluster
from hadoop_cluster import HadoopCluster

import ec2_cluster
import hadoop_cluster
